{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5303a36c",
   "metadata": {},
   "source": [
    "## Asignment: Computer vison course\n",
    "- Alumno 1: Daniel Carmona Pedrajas\n",
    "- Alumno 2: Joel Pardo Ferrera\n",
    "\n",
    "The goals of this assignment are:\n",
    "+ Develop proficiency in using Tensorflow/Keras for training neural nets (NNs).\n",
    "+ Put in practice Unit 4 knowledge to optimize the parameters and architecture of a feed-forward Neural Net (ffNN), in the context of a computer vision problem.\n",
    "+ Use NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (cNN).\n",
    "\n",
    "Follow the link above to download the classification benchmark termed “xview_recognition”: [https://drive.upm.es/s/c4gfDOOwWfPdm0z](https://drive.upm.es/s/c4gfDOOwWfPdm0z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b895ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "tf.device('/device:GPU.0')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"../../../ImagenesPractica/xview_recognition/\"\n",
    "MODELS_PATH = \"../Models/\"\n",
    "MODELS_HISTORY_PATH = f\"{MODELS_PATH}models_history.csv\"\n",
    "MODELS_IMAGES_PATH = f\"{MODELS_PATH}ModelsImages/\"\n",
    "RESULTS_IMAGES_PATH = \"../ResultsImages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {13: 'CARGO_PLANE', \n",
    "              15: 'HELICOPTER', \n",
    "              18: 'SMALL_CAR', \n",
    "              19: 'BUS', \n",
    "              23: 'TRUCK', \n",
    "              41: 'MOTORBOAT', \n",
    "              47: 'FISHING_VESSEL', \n",
    "              60: 'DUMP_TRUCK', \n",
    "              64: 'EXCAVATOR', \n",
    "              73: 'BUILDING',\n",
    "              86: 'STORAGE_TANK', \n",
    "              91: 'SHIPPING_CONTAINER'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    src_raster = rasterio.open(filename, 'r')\n",
    "    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img\n",
    "\n",
    "def generator_images(objs, batch_size, do_shuffle=False):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n",
    "        for group in groups:\n",
    "            images, labels = [], []\n",
    "            for (filename, obj) in group:\n",
    "                # Load image\n",
    "                images.append(load_geoimage(filename))\n",
    "                probabilities = np.zeros(len(categories))\n",
    "                probabilities[list(categories.values()).index(obj.category)] = 1\n",
    "                labels.append(probabilities)\n",
    "            images = np.array(images).astype(np.float32)\n",
    "            labels = np.array(labels).astype(np.float32)\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ecfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw_confusion_matrix(cm, categories, model_name, normalize=False):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.show(fig)\n",
    "    fig.savefig(f\"{RESULTS_IMAGES_PATH}cm_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e97b0a",
   "metadata": {},
   "source": [
    "## Training\n",
    "Design and train a ffNN to deal with the xview_recognition classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load database\n",
    "json_file = f'{IMAGES_PATH}xview_ann_train.json'\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c13805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "counts = dict.fromkeys(categories.values(), 0)\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'], json_data['annotations']):\n",
    "    image = GenericImage(IMAGES_PATH + json_img['file_name'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.id = json_ann['id']\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = list(categories.values())[json_ann['category_id']-1]\n",
    "    # Resampling strategy to reduce training time\n",
    "    if counts[obj.category] >= 1000:\n",
    "        continue\n",
    "    counts[obj.category] += 1\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)\n",
    "    \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b74d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "anns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47627995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "exp = \"-2-1-1\"\n",
    "ACTIVATION = \"relu\"\n",
    "DROPOUT = \"No\"\n",
    "REGULARIZATION = \"No\"\n",
    "INITIALIZER = \"No\"\n",
    "BATCH_NORMALIZATION = \"No\"\n",
    "\n",
    "def initial_model():\n",
    "    print('Load model')\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(224, 224, 3)))\n",
    "    model.add(Activation(ACTIVATION))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(categories)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model_name = f\"ir_12{exp}\"\n",
    "    return model, model_name\n",
    "\n",
    "def ffnn_arquitecture1():\n",
    "    print('Load model')\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(224, 224, 3)))\n",
    "    n_hidden = [256, 128, 64, 32]\n",
    "    for neurons in n_hidden:\n",
    "        model.add(Dense(len(neurons)))\n",
    "        model.add(Activation(ACTIVATION))\n",
    "    model.add(Dense(len(categories)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model_name = f\"ir_256_128_64_32{exp}\"\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load architecture\n",
    "model, model_name = initial_model()\n",
    "model.summary()\n",
    "plot_model(model, to_file=f\"{MODELS_IMAGES_PATH}{model_name}.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8df471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is changed to 0.001\n",
    "from keras.optimizers import Adam\n",
    "OPTIMIZER_NAME = \"Adam 0.9 0.999\"\n",
    "LEARNING_RATE = 0.001\n",
    "opt = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.00, amsgrad=True, clipnorm=1.0, clipvalue=0.5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "model_checkpoint = ModelCheckpoint(f'{MODELS_PATH}{model_name}.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]\n",
    "CALLBACKS_NAMES = \"reducelr 10 0.1 - early_stop 40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e730383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "# Generators\n",
    "BATCH_SIZE = 32\n",
    "train_generator = generator_images(objs_train, BATCH_SIZE, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, BATCH_SIZE, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c433bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print('Training model')\n",
    "N_EPOCHS = 20\n",
    "train_steps = math.ceil(len(objs_train)/BATCH_SIZE)\n",
    "valid_steps = math.ceil(len(objs_valid)/BATCH_SIZE)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "h = model.fit_generator(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=N_EPOCHS, callbacks=callbacks, verbose=1)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "TRAINING_TIME = round(end_time - start_time)\n",
    "\n",
    "# Best validation model\n",
    "best_idx = int(np.argmax(h.history['val_accuracy']))\n",
    "best_value = np.max(h.history['val_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37797e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results=pd.DataFrame(h.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.savefig(f\"{RESULTS_IMAGES_PATH}tr_{model_name}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87725ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load database\n",
    "json_file = f'{IMAGES_PATH}xview_ann_test.json'\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41572c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'], json_data['annotations']):\n",
    "    image = GenericImage(IMAGES_PATH + json_img['file_name'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.id = json_ann['id']\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = list(categories.values())[json_ann['category_id']-1]\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "model = keras.models.load_model(f'{MODELS_PATH}{model_name}.hdf5')\n",
    "y_true, y_pred = [], []\n",
    "for ann in anns:\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        y_true.append(obj_pred.category)\n",
    "        y_pred.append(pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories, model_name, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "\n",
    "MEAN_ACCURACY = (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100)\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "MEAN_RECALL = (acc.mean() * 100)\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "MEAN_PRECISION = (acc.mean() * 100)\n",
    "\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4beab",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a628584",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.read_csv(MODELS_HISTORY_PATH, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    'model_name',\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'optimizer',\n",
    "    'activation',\n",
    "    'batch_size',\n",
    "    'callbacks',\n",
    "    'regularization',\n",
    "    'initializer',\n",
    "    'dropout',\n",
    "    'batch_normalization',\n",
    "    'mean_accuracy',\n",
    "    'mean_recall',\n",
    "    'mean_precision',\n",
    "    'training_time'\n",
    "]\n",
    "\n",
    "new_history = [[\n",
    "    model_name,\n",
    "    N_EPOCHS,\n",
    "    LEARNING_RATE,\n",
    "    OPTIMIZER_NAME,\n",
    "    ACTIVATION,\n",
    "    BATCH_SIZE,\n",
    "    CALLBACKS_NAMES,\n",
    "    REGULARIZATION,\n",
    "    INITIALIZER,\n",
    "    DROPOUT,\n",
    "    BATCH_NORMALIZATION,\n",
    "    MEAN_ACCURACY,\n",
    "    MEAN_RECALL,\n",
    "    MEAN_PRECISION,\n",
    "    TRAINING_TIME\n",
    "    ]]\n",
    "\n",
    "new_history_df = pd.DataFrame(new_history, columns=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenation = pd.concat([history_df, new_history_df], ignore_index=True)\n",
    "concatenation.to_csv(MODELS_HISTORY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2b169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
